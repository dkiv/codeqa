# config.yaml
# Central configuration for repo paths, chunking parameters, retrieval settings, and model choices.
# config.yaml â€” central configuration for CodeQA RAG

# --- Ingestion settings (used by ingest.py) ---
repo_path: data/repo
index_path: data/index
file_globs:
  - "**/*.py"
  - "**/*.md"
  - "**/*.pdf"
  - "**/*.docx"
exclude_globs:
  - "**/.git/**"
  - "**/__pycache__/**"
  - "**/node_modules/**"
  - "**/dist/**"
  - "**/build/**"
  - "**/.venv/**"
chunk:
  size: 800
  overlap: 120
  language: python
model:
  embedding: BAAI/bge-small-en

# --- Retrieval/answer settings (used by rag_chain.py) ---
# rag_chain.py will merge these with its DEFAULT_CONFIG
# (If you omit a key, the default stays in effect.)
# Ensure this index_path matches the one above.
index_path: data/index
model:
  embedding: BAAI/bge-small-en
  chat: gpt-4o-mini
  temperature: 0.0
retrieval:
  k: 8
  max_context_chunks: 6
  max_chunk_chars: 1200
  search_type: mmr         # try MMR to reduce redundancy in results
  mmr_fetch_k: 40
  mmr_lambda: 0.5


# if you want to use OpenAI API for chat model, set this to true
# (Otherwise, it will just print the prompt and exit.)
# Note: this requires you to set OPENAI_API_KEY in your environment.
app:
  use_openai_api: false

# --- Privacy (request-time pseudonymization) ---
privacy:
  request_time: true         # scrub retrieved context + question before LLM call
  enable_regex: true         # use regex patterns for structured PII (EMAIL/PHONE/SSN/CREDIT_CARD/DATE/POSTAL_CODE/ADDRESS)
  enable_ner: true          # if spaCy (and a model) is installed, set true to include PERSON/ORG/GPE
  entity_types: [PERSON, ORG, GPE, EMAIL, PHONE, SSN, CREDIT_CARD, DATE, POSTAL_CODE, ADDRESS]

# --- Ingest behavior ---
# If true, remove the existing index directory before rebuilding to avoid duplicates
reset_index: true
